\documentclass[10pt]{article}

\input{/Users/gabesekeres/Dropbox/LaTeX_Docs/pset_preamble.tex}

\course{ECON 6200}
\pset{2}
\begin{document}
\maketitle


\begin{enumerate}
	\item We have that $Y \sim U[a,b]$ \begin{enumerate} \item Recall that the OLS estimator in the simple mean case is defined as \[\hat{\mu}_{OLS} \in \argmin_{m} (Y - m)^2 \]Since $Y$ is scalar-valued and i.i.d., we can consider a single scalar case in which \[\hat{\mu}_{OLS} \in \argmin_m (Y_i - m)^2\]which admits the first order condition on $m$\[2(Y_i - m) = 0 \Longrightarrow \frac{1}{n}\sum_{i=1}^n m = \frac{1}{n}\sum_{i=1}^n Y_i \Longrightarrow \hat{\mu}_{OLS} = \bar{Y}\] \item First, to show that $\hat{\mu}_{\text{order}}$ is unbiased, it suffices to show that $\expect[\hat{\mu}_{\text{order}}] = \expect[Y] = \mu$. Notice that from the moments of order statistics,\[\expect[Y\opt_1] = a + \frac{b-a}{n+1} \qquad \text{ and } \qquad \expect[Y\opt_n] = b - \frac{b-a}{n+1} \]Thus, we have that\[\expect[\hat{\mu}_{\text{order}}] = \frac{\expect[Y_1\opt + Y_n\opt]}{2} = \frac{1}{2}\expect\barl a + \frac{b-a}{n+1} + b -\frac{b-a}{n+1}\barr = \frac{b+a}{2} = \expect[Y]\]from the properties of uniform distributions. Next, recall that the variance of a $Y$ is \[\sigma^2 = \frac{(a-b)^2}{12}\]This means that\[\var(\bar{Y}) = \var\parl \frac{1}{n}\sum_{i=1}^n Y_i\parr \overset{\text{i.i.d}}{=} \frac{1}{n^2} n\sigma^2 = \frac{(a-b)^2}{12n}\]Noting that the sample minimum and maximum are each order statistics, and using the properties of the 1st and $n$th order statistics, we have that\begin{align*}\var(Y_1\opt) &= \frac{n(b-a)^2}{(n+1)^2(n+2)} \\ \var(Y_n\opt) &= \frac{n(b-a)^2}{(n+1)^2(n+2)} \\ \cov(Y_1\opt,Y_2\opt) &= \frac{(b-a)^2}{(n+1)^2(n+2)}\end{align*}Then, we have directly that\[\var(\hat{\mu}_{\text{order}}) = \frac{\var(Y_1\opt) + \var(Y_n\opt) + 2\cov(Y_1\opt,Y_n\opt)}{4} = \frac{(b-a)^2}{2(n+1)(n+2)} \]and for all positive integers $n$, \[2(n+1)(n+2) = 2n^2 + 6n + 4 \ge 12n \Longleftrightarrow (n-2)(n-1) \ge 0\] so this estimator has weakly lower variance, and has strictly lower variance for any $n \ge 3$. \item The Gauss-Markov Theorem \emph{does} apply to $\bar{Y}$, as the assumptions (linearity, unbiasedness, strong exogeneity, and the rank condition) hold. However, though it would seem that we had found an unbiased linear estimator with lower variance than the BLUE, we actually haven't, since $\hat{\mu}_{\text{order}}$ is \emph{not} linear in $Y$. This can be clearly seen by the fact that the covariance term of its variance is positive, meaning that as $Y\opt_1$ increases, $Y\opt_n$ increases, and vice versa, so a linear transformation of the terms of $\hat{\mu}_{\text{order}}$ will have an oversized effect on $\hat{\mu}_{\text{order}}$, so it is not linear.\end{enumerate}
	\item Consider simple linear regression both without a constant \[Y = \beta \cdot X + \varepsilon\]and with a constant \[Y = \alpha + \beta \cdot X + \varepsilon\] \begin{enumerate} \item Recall that $\tilde{\beta}$ solves the least-squares minimization problem: \[\tilde{\beta} \in \argmin_b (Y - Xb)^2\]which admits the first order condition \[-2 X'Y + 2(X'X)b = 0\]And in finite sample, this becomes\[\tilde{\beta} = \frac{\sum_{i=1}^n X_iY_i}{\sum_{i=1}^n X_i^2}\] \item Recall from lecture the analogous closed form of $\hat{\beta}$:\[\hat{\beta} = \frac{\sum_{i=1}^n (X_i - \bar{X})Y_i}{\sum_{i=1}^n (X_i - \bar{X})X_i}\]So, as always, we have that (from exogeneity)\[\expect[\hat{\beta}] = \expect\barl \frac{\sum_{i=1}^n (X_i - \bar{X})(X_i\beta + \varepsilon)}{\sum_{i=1}^n (X_i - \bar{X})X_i}\barr = \beta + \underbrace{\expect\barl\frac{\sum_{i=1}^n X_i\varepsilon}{\sum_{i=1}^n (X_i - \bar{X})^2}\barr}_{=0} = \beta\]and\[\var(\hat{\beta}) = \var\parl \beta + \frac{\sum_{i=1}^n X_i\varepsilon}{\sum_{i=1}^n (X_i - \bar{X})X_i}\parr = \frac{\sigma^2}{\sum_{i=1}^n (X_i - \bar{X})^2}\]Using our closed form for $\tilde{\beta}$, we get that (from exogeneity)\[\expect[\tilde{\beta}] = \expect\barl \frac{\sum_{i=1}^n X_i(X_i\beta + \varepsilon)}{\sum_{i=1}^n X_i^2}\barr = \beta + \underbrace{\expect\barl\frac{\sum_{i=1}^n X_i\varepsilon}{\sum_{i=1}^n X_i^2}\barr}_{=0} = \beta\]and\[\var(\tilde{\beta}) = \var\parl \frac{\sum_{i=1}^n X_i\varepsilon}{\sum_{i=1}^n X_i^2}\parr = \frac{\sigma^2}{\sum_{i=1}^n X_i^2}\]So the variance is weakly smaller, strictly as long as $\bar{X} \ne 0$. \item First, assume that the true model is \[Y = \alpha + \beta X + \varepsilon\]but that we use the regression \[Y = \beta X + \varepsilon\]As we saw above, our slope coefficient is\[\tilde{\beta} = \frac{\sum_{i=1}^n X_iY_i}{\sum_{i=1}^n X_i^2} = \frac{\sum_{i=1}^n X_i(\alpha + \beta X_i + \varepsilon)}{\sum_{i=1}^n X_i^2} = \beta + \frac{\alpha \sum_{i=1}^n X_i}{\sum_{i=1}^n X_i^2} + \frac{\sum_{i=1}^n X_i\varepsilon}{\sum_{i=1}^n X_i^2}\]so we have that\[\expect[\tilde{\beta}\mid X] = \expect \barl\beta + \frac{\alpha \sum_{i=1}^n X_i}{\sum_{i=1}^n X_i^2} + \frac{\sum_{i=1}^n X_i\varepsilon}{\sum_{i=1}^n X_i^2}\barr = \beta + \alpha \frac{\sum_{i=1}^n X_i}{\sum_{i=1}^n X_i^2}\]so $\tilde{\beta}$ is unbiased only if $\sum_{i=1}^n X_i = 0$, the class of models in which $X$ is already demeaned. This allows us to state a modification of Gauss-Markov: \begin{theorem} If the previous assumptions (linearity, strong exogeneity, rank condition, and spherical errors) hold, and $\tilde{\beta}$ is a linear unbiased estimator for a model where $\alpha = 0$, then \[\var(\tilde{\beta} \mid X) \ge \sigma^2 (X'X)^{-1}\] \end{theorem} \begin{proof} Assume that $\tilde{\beta} = CY$ for some matrix $C$, and since $\tilde{\beta}$ is unbiased it must be that $CX = I$. Thus, we write \[C = (X'X)^{-1}X' + D\quad \text{where } D = C - (X'X)^{-1}X'\]As in class, unbiasedness requires that \[\expect[((X'X)^{-1}X' + D)Y \mid X] = \beta \Longrightarrow \expect[DX \beta \mid X] = 0 \Longrightarrow DX = 0\]Thus, we have that\begin{align*} \var(\tilde{\beta} \mid X) &= \var(((X'X)^{-1} X' + D)Y \mid X) \\ &=  \var(((X'X)^{-1} X' + D)(X \beta + \varepsilon) \mid X)\\ &= \var(((X'X)^{-1} X' + D) \varepsilon\mid X) \\ &= \sigma^2 ((X'X)^{-1} X' + D)((X'X)^{-1} X' + D)' \\ &= \sigma^2 ( (X'X)^{-1}X'X(X'X)^{-1} + \underbrace{DX(X'X)^{-1}}_{=0} + \underbrace{(X'X)^{-1}D'X'}_{=0} + D'D)\\ &\ge \sigma^2 (X'X)^{-1}\end{align*}where conclusion follows from the fact that $D'D$ is positive semi-definite.\end{proof}\end{enumerate}
	\item Estimate the efficacy of a vaccine. \begin{enumerate} \item Our estimator is the simple sample average: \[\hat{\pi} = \frac{9}{178} \approx 0.05056\]A Wald 95\% confidence interval is constructed with the bounds\[\hat{\pi} \pm z_{0.975} \sqrt{\frac{\hat{\pi}(1-\hat{\pi})}{n}} \]so we have the approximate confidence interval \[\pi \in (\hat{\pi} - 0.0322, \hat{\pi} + 0.0322) \approx (0.0184,0.0827)\]We can alternatively construct a Poisson approximation by taking $\lambda = 178\pi$ and using test inversion on the Poisson distribution where $k = X = 9$. Then, the formula for our (Poisson) confidence interval is \[\lambda \in \parl \frac{1}{2}\chi^2_{18,0.025}, \frac{1}{2}\chi^2_{20,0.975}\parr \approx (4.12, 17.09)\]and since $\lambda = 178\pi$, we have the confidence interval \[\pi \in (0.0231, 0.0960)\]Finally, using the relationship between the $F$ distribution and the binomial distribution (which I googled), we can construct an exact inversion of the binomial distribution, and get a confidence interval defined by, for $k = 9$, $n = 178$, and $\alpha = 0.05$,\[\parl \frac{k}{k + (n-k+1)F_{1-\alpha/2;2(n-k+1),2k}}, \frac{(k+1)F_{1-\alpha/2;2(k+1),2(n-k)}}{(n-k) + (k+1)F_{1-\alpha/2;2(k+1),2(n-k)}}\parr\]We have that, from a distribution table,\begin{align*} F_{0.975;340,18} &\approx 2.2117 \\ F_{0.975;20,338} &\approx 1.7495 \end{align*} so this becomes \[\parl \frac{9}{9 + 170 \cdot 2.2117}, \frac{10 \cdot 1.7495}{169 + 10 \cdot 1.7495}\parr \approx (0.0234, 0.0938)\]In the last two, we are essentially finding $\underline{\lambda},\overline{\lambda}$ such that when the Poisson distribution takes a value $k=9$, we are sure with 95\% confidence that the true $\lambda$ is within those values. We are doing the same with the binomial inversion -- finding $\underline{\pi},\overline{\pi}$ such that since we observed $9$ successes of 178 trials, we are sure with 95\% confidence that $\pi$ is within those bounds. \item \begin{proof} We have that $[\underline{\theta},\overline{\theta}]$ is a valid confidence interval for $\theta$ and that $f(\cdot)$ is a known monotonic function. WLOG, assume that $f$ is monotonically increasing. We know that since this confidence interval is valid,\[\prob\{\theta \in [\underline{\theta},\overline{\theta}]\} = 1-\alpha \Longrightarrow \prob\{\underline{\theta} \le \theta \le \overline{\theta}\} = 1-\alpha\]Since $f$ is monotonically increasing, $f(x) \le f(y) \Longrightarrow x \le y$. Thus, \[\underline{\theta} \le \theta \le \overline{\theta} \Longleftrightarrow f(\underline{\theta}) \le f(\theta) \le f(\overline{\theta})\]so we have that \[\prob\{f(\theta) \in [f(\underline{\theta}),f(\overline{\theta})]\} = 1-\alpha\]so $[f(\underline{\theta}),f(\overline{\theta})]$ is a $1-\alpha$ confidence interval for $f(\theta)$. The same logic applies, reversing the inequalities, if $f$ is monotonically decreasing. \end{proof} \item If we know $\rho = \prob\{\text{vaccinated}\}$, then by Bayes' Theorem, we have that\[VE = 1 - \frac{\frac{\pi \cdot \prob\{\text{infected}\}}{\rho}}{\frac{(1-\pi) \cdot \prob\{\text{infected}\}}{1-\rho}} = 1 - \frac{\pi(1-\rho)}{(1-\pi)\rho}\]so \[\frac{\partial}{\partial \pi} VE = \frac{\rho - 1}{\rho} \frac{1}{(1-\pi)^2}\]and $VE$ is strictly monotonic since for $\pi,\rho \in (0,1)$ the partial with respect to $\pi$ is strictly negative. \item We have that \[\rho = \frac{18,559}{18,559 + 18,708} \approx 0.498\]Thus, \[VE = 1 - \frac{0.502 \cdot \pi}{0.498 \cdot (1-\pi)}\]which is a strictly monotonic (decreasing) function of $\pi$. Thus, we have that our estimator is\[\hat{VE} = 1 - \frac{0.502 \cdot \hat{\pi}}{0.498 \cdot (1-\hat{\pi})} \approx 0.946\]From part (b), we have that we can simply transform our confidence intervals using the monotonically decreasing function \[f(x) = 1 - \frac{0.502 \cdot x}{0.498 \cdot (1-x)}\]We get that the Wald confidence interval is\[(f(0.0827),f(0.0184)) \approx (0.9091, 0.9811)\]The Poisson approximation confidence interval is\[(f(0.0960),f(0.0231)) \approx (0.8930, 0.9762)\]Finally, the binomial confidence interval is \[(f(0.0938),f(0.0234)) \approx (0.8957,0.9758)\] \item Yes! We can see the binomial confidence interval in the overall vaccine efficacy line, and their estimator has the same mean as ours. They cite the exact same Clopper and Pearson method I used! \end{enumerate}
\end{enumerate}







\end{document}
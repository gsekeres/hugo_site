\documentclass[10pt]{article}

\input{/Users/gabesekeres/Dropbox/LaTeX_Docs/pset_preamble.tex}

\course{ECON 6200}
\pset{5}
\begin{document}
\maketitle

\begin{enumerate}
	\item Extremum Estimation \begin{enumerate} \item Here, we do not assume that $\theta_0$ is a well-separated optimum, and we do not assume uniform convergence. We do assume (above the class assumptions about consistency) that $m$ is strictly convex. Strict convexity does imply the above, as combined with continuity we will have directly that the minimum is well-separated and that the sample $m_n$ converge uniformly to $m$ as long as they converge pointwise. \item \begin{proof} It suffices to show that \[\Bigg| \frac{1}{n}\sum_i m(W_i,\theta) - \expect m(W,\theta)\Bigg| \toprob 0 \forall \theta \in \reals \]Fix some $\theta \in \reals$, and recall that we are assuming that $\expect m(W,\theta) \in \reals$. If $m$ is continuous in some open neighborhood of $\theta$, conclusion follows immediately from the continuous mapping theorem. Therefore, assume that $m$ has a discontinuity at exactly $\theta$. Since $m$ is strictly convex, it must be that $m(W,\theta) = \lim_{W_n\searrow W} m(W_n,\theta)$, \ie\;as $W$ is approached from above. Conclusion then directly follows from Jensen's Inequality, since $\frac{1}{n}\sum m(W_i,\theta) \ge \expect m(W,\theta)$ almost surely as $n$ increases, and so is approaching from above where the discontinuity will have no bite. \end{proof} \end{enumerate}
	\item Worked Example with Geometric Distribution \begin{enumerate} \item The random variable $X$ will equal $x$ in precisely the case where there are $x-1$ failures followed by one success. Since the trials are i.i.d., we have immediately that \[\prob(X = x) = (1-\pi)^{x-1}\pi\] Since the geometric distribution is memoryless, we have that \[\expect X = \pi \cdot \expect[X \mid X = 1] + (1-\pi) \cdot \expect[X \mid X > 1] = \pi + (1-\pi) \cdot (\expect X + 1)\]\[\Longrightarrow \expect X = \frac{1}{\pi}\]We have that \[\expect X^2 = \pi + (1-\pi)(\expect (X + 1)^2) = \pi + (1-\pi) (\expect X^2 + 2\expect X + 1)\]\[\Longrightarrow \expect X^2 = \frac{2(1-\pi)}{\pi^2} + \frac{1}{\pi}\]So\[\var(X) = \expect X^2 - (\expect X)^2 = \frac{2(1-\pi)}{\pi^2} + \frac{1}{\pi} - \frac{1}{\pi^2} = \frac{1-\pi}{\pi^2}\] \item Given data $\{x_i\}$, the likelihood function is \[L_n(\pi) = \prod_{i=1}^n (1-\pi)^{x_i - 1} \pi\]so the log likelihood function is \[\ell_n(\pi) = n \log \pi + \sum_{i=1}^n (x_i - 1)\log (1-\pi)\]Thus, \[\hat{\pi}_{MLE} = \argmax_{\pi \in [0,1]}  n \log \pi + \sum_{i=1}^n (x_i - 1)\log (1-\pi)\]Since this is concave by inspection, the maximum is attained and precisely characterized by the first order condition:\[ \frac{n}{\pi} - \frac{\sum_{i=1}^n (x_i - 1)}{1-\pi} = 0 \Longrightarrow \hat{\pi}_{MLE} = \frac{1}{\bar{X}}\]We can use the delta method which will apply as long as $\pi \ne 0$. We have that \[\sqrt{n}(\bar{X} - \expect X) \todist \normal(0,\var(X)) = \normal\parl 0,\frac{1-\pi}{\pi^2}\parr \]So using $g(x) = \frac{1}{x}$, which admits $g'(x) = -x^{-2}$ we have that\[\sqrt{n}(\hat{\pi}_{MLE} - \pi) = \sqrt{n}(g(\bar{X}) - g(\expect(X))) \todist \normal\parl 0, \frac{1-\pi}{\pi^2} \cdot (-\pi^2)^2\parr = \normal(0,\pi^2(1-\pi))\] \item This estimator is not unbiased. The function $\frac{1}{x}$ is convex for all positive $n$, so from Jensen's Inequality we have that $\expect[\hat{\pi}_{MLE}] \ge \pi$, where strict inequality holds except when $\pi=1$. Thus, the estimator is biased upwards. \end{enumerate}
	\item Uniform Distribution \begin{enumerate} \item We will use the simple moment function $g(x,\mu) = x - \mu$. Then, we have that $\expect g(X,\mu)= \expect X - \mu$, which equals zero only when $\mu = \expect X = \mu_0$. The sample moment function is directly $g_n(\mu) = \frac{1}{n}\sum_{i=1}^n g(X_i,\mu) = \bar{x}_n - \mu$ so the GMM estimator is clearly the sample average. We can characterize the asymptotics using the central limit theorem:\[\sqrt{n}(\hat{\mu}-\mu_0) \todist \normal(0,\var(X)) = \normal\parl 0,\frac{(\beta-\alpha)^2}{12}\parr\] \item We first verify the assumptions. (1) follows from the law of large numbers, (2) we have that $W = \hat{W}= 1$, which is trivially symmetric and positive definite, (3) we have that $\mu_0 \in [\alpha,\beta] \subset \interior(\reals)$, (4) we have that since $g(\cdot)$ is linear in $\mu$, $g(x,\mu) \in \cont^\infty$ for any $\mu \in \reals$, including $\mu_0$. For (5) we have from the central limit theorem that $S = \frac{(\beta-\alpha)^2}{12}$, which is positive. For (6), take a sequence $\mu_n \to \mu_0$, and we have that $\frac{\partial g(x,\mu_n)}{\partial \mu} = -1 \toprob -1 = G(\mu_0) = \expect \frac{\partial g(x,\mu_0)}{\partial \mu}$, and (7) we know that singletons always have full column rank. Thus, the assumptions hold, and we have that $W = 1$, $S = \frac{(\beta-\alpha)^2}{12}$, and $G = -1$. \item  Recall that the density of a uniform distribution is $\ones_{x\in[\alpha,\beta]} \frac{1}{\beta-\alpha}$. The likelihood function with $n$ i.i.d. observations is therefore\[L_n(\alpha,\beta) = \prod_{i=1}^n \frac{1}{\beta-\alpha} \ones\{x_i \in [\alpha,\beta]\} = (\beta-\alpha)^{-n} \ones\{\alpha \le x_{[1]} \le x_{[n]} \le \beta\}\]Observe that maximizing the likelihood is equivalent to minimizing the length of $\beta - \alpha$ subject to $\beta \ge x_{[n]}$ and $\alpha \le x_{[1]}$. Thus, clearly the likelihood is maximized at equality, so\[\hat{\alpha}_{ML} = x_{[1]} \qquad ; \qquad \hat{\beta}_{ML} = x_{[n]}\] \item We know from above that $\expect \hat{\mu} = \expect \frac{1}{n}\sum_{i=1}^n x_i = \frac{n}{n}\expect \bar{x}_n = \mu_0$, so $\hat{\mu}$ is unbiased. We also have that \[\var(\hat{\mu}) = \frac{1}{n^2} n \var(X) = \frac{(\beta-\alpha)^2}{12n}\]So the mean squared error is:\[\mse(\hat{\mu}) = \bias(\hat{\mu})^2 + \var(\hat{\mu}) = \frac{(\beta-\alpha)^2}{12n}\]We have that\[\expect \tilde{\mu} = \frac{\expect x_{[1]} + \expect x_{[n]}}{2} = \frac{\alpha + \beta}{2} = \mu_0\]so $\tilde{\mu}$ is unbiased. We also have that \[\var(\tilde{\mu}) = \frac{1}{4} \parl \var(x_{[1]}) + \var(x_{[n]})\parr = \frac{n (\beta-\alpha)^2}{2(n+1)^2(n+2)}\]So the mean squared error is:\[\mse(\tilde{\mu}) = \bias(\tilde{\mu})^2 + \var(\tilde{\mu}) =  \frac{n (\beta-\alpha)^2}{2(n+1)^2(n+2)}\] \item The Gauss-Markov Theorem does apply to $\hat{\mu}$, and $\hat{\mu}$ does attain the BLUE variance. However, $\tilde{\mu}$ is not a linear estimator, since there is correlation between $\hat{\alpha}_{ML}$ and $\hat{\beta}_{ML}$. It is unbiased and (asymptotically) attains a lower variance, but is not linear. \item The corresponding population criterion is \[Q(\mu) = (\beta-\mu)^2 + (\mu - \alpha)^2\]The consistency theorem for convex $Q$ applies here, since this function is strictly convex in the argument, $\mu_0$ is in the interior and uniquely minimizes $Q$, the sample criterion functions are convex, and $Q_n$ converges pointwise to $Q$ for any $\mu$. \item Note that\[\tilde{\mu} - \mu_0 = \frac{\hat{\alpha}_{ML} - \alpha + \hat{\beta}_{ML} - \beta}{2} \]Thus, since $\sqrt{n}(\hat{\alpha}_{ML} - \alpha)$ and $\sqrt{n}(\hat{\beta}_{ML} - \beta)$ are both asymptotically normal, $\sqrt{n}(\tilde{\mu} - \mu_0)$ is asymptotically the composition of normals, which since we have correlation is not asymptotically normal. I can go no further than this. \item The theorem is not contradicted because the Hessian is not nonsingular everywhere, and we can improve asymptotically when $\mu_0 = 0$.\end{enumerate}
	\item GMM versus ML in a Classic Application \begin{enumerate} \item They say that GMM is advantaged over ML because the moment functions are functions of true preference parameters $\alpha$ and $\beta$. ML would require those to be functions of policy, while Lucas says that such parameters are policy-invariant. \item The moment conditions are that there is no improvement to be made in expectation, basically that markets will clear and there is no arbitrage or other free lunch. \item $b_T\opt$ is optimal in the sense of all estimators based on the specific moment conditions in this problem, but does not attain Cram\'er-Rao efficiency or other general bounds. \item We could match the efficiency of ML, as long as we made all of the distributional assumptions that the authors don't want to. In that case, we would be numerically identical to the ML estimator. However, we would inherit exactly ML's disadvantages from part (a). \item $\alpha$ is the coefficient of relative risk aversion and $\beta$ is the discount factor. The estimates seem relatives plausible for $\alpha$ and $\beta$ both -- people are relatively risk-averse and relatively patient. The variation between specifications makes sense to me -- $\alpha$ is very sensitive to specification, but $\beta$ barely changes at all. That intuitively holds.\end{enumerate}
\end{enumerate}








\end{document}